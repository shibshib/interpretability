{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from exceptions.Exceptions import ModelNotFoundException\n",
    "from models import *\n",
    "from models.custom_data_parallel import CustomDataParallel\n",
    "from loguru import logger\n",
    "from torch.autograd import Variable\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "trainloader = None\n",
    "testloader = None\n",
    "learning_rate = 0.01\n",
    "ckpt_path = \"./checkpoint/\"\n",
    "\n",
    "models = {\n",
    "        \"RegNetX_200MF\": RegNetX_200MF(),\n",
    "        \"VGG\": VGG('VGG19'),\n",
    "        \"ResNet18\": ResNet18(),\n",
    "        \"PreActResNet18\": PreActResNet18(),\n",
    "        \"GoogLeNet\": GoogLeNet(),\n",
    "        \"DenseNet121\": DenseNet121(),\n",
    "        \"ResNeXt29_2x64d\": ResNeXt29_2x64d(),\n",
    "        \"MobileNet\": MobileNet(),\n",
    "        \"MobileNetV2\": MobileNetV2(),\n",
    "        \"DPN92\": DPN92(),\n",
    "#        \"ShuffleNetG2\": ShuffleNetG2(),\n",
    "        \"SENet18\": SENet18(),\n",
    "#        \"ShuffleNetV2\": ShuffleNetV2(),\n",
    "        \"EfficientNetB0\": EfficientNetB0()\n",
    "    }\n",
    "\n",
    "# Some hyperparams\n",
    "model_name = 'RegNetX_200MF'\n",
    "net = RegNetX_200MF()\n",
    "num_epochs = 15\n",
    "start_epoch = 0\n",
    "learning_rate = 0.01\n",
    "train_batch_size = 128\n",
    "test_batch_size = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up parameters and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parallel.data_parallel.DataParallel'>\n"
     ]
    }
   ],
   "source": [
    "# Set up optimizer\n",
    "learning_rate = learning_rate\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate,\n",
    "                    momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "                    step_size=10.0, gamma=0.1)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "ckpt_path = ckpt_path + model_name + \".torch\"\n",
    "num_epochs = num_epochs\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = CustomDataParallel(net)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-24 15:53:58.929 | INFO     | __main__:<module>:1 - ==> Preparing data..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "logger.info('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=test_batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test functions, and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        log_interval = 10\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total = total + targets.size(0)\n",
    "            \n",
    "            max_logit = outputs.data.max(1, keepdim=True)[1]\n",
    "            correct = max_logit.eq( targets.data.view_as(max_logit) ).sum()\n",
    "            processed = ((batch_idx + 1) * train_batch_size)\n",
    "            \n",
    "            accuracy = (100. * correct) / train_batch_size\n",
    "            progress = (100. * processed) / len(trainset)\n",
    "\n",
    "            if (batch_idx + 1) % log_interval == 0: \n",
    "                    logger.info('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLocal Loss: {:.6f}\\t Accuracy: {:.6f}\\t', \n",
    "                    epoch, processed, len(trainset), progress, loss.item(), accuracy)\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "    acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            acc = 100.*correct/total\n",
    "\n",
    "        logger.info('Test set: loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(loss.item(), correct, len(testset), acc))  \n",
    "\n",
    "        if acc > best_acc:\n",
    "            print('Saving..')\n",
    "            state = {\n",
    "                'net': net.state_dict(),\n",
    "                'acc': acc,\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "            if not os.path.isdir('checkpoint'):\n",
    "                os.mkdir('checkpoint')\n",
    "            \n",
    "            logger.info( 'Saving/Serving model: epoch: {}, loss: {}, path: {}', epoch, loss.item(), './checkpoint' )\n",
    "            torch.save( {'epoch': epoch,\n",
    "                        'model_state_dict': net.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': loss.item()}, ckpt_path)\n",
    "            best_acc = acc\n",
    "\n",
    "def train_model(start_epoch):\n",
    "    for epoch in range(start_epoch, start_epoch+num_epochs):\n",
    "        train(epoch)\n",
    "        scheduler.step()\n",
    "        test(epoch)\n",
    "\n",
    "# Start training model...\n",
    "#train_model(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(ckpt_path)\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "#net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's grab the test set and input it through the model, \n",
    "# and train an SVM on the output of the final layer. \n",
    "\n",
    "# Principal component analysis.\n",
    "pca = PCA(n_components=2)\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        correct = 0\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        import pdb; pdb.set_trace()\n",
    "        for input, target in zip(inputs, targets):\n",
    "            output = net()\n",
    "            print(net.final_layer_output)\n",
    "            predicted = output.data.max(1, keepdim=True)[1]\n",
    "            \n",
    "#            if torch.eq(predicted, target):\n",
    "#                # Samples correctly classified, let's grab the output of the last layer \n",
    "#                # to input it into the SVM.\n",
    "#                final_layer_output = net.final_layer_output\n",
    "#                final_layer_output = final_layer_output.view(-1, 23552)\n",
    "#                final_layer_output = final_layer_output.tolist()\n",
    "#                X.extend(final_layer_output)\n",
    "#                 Y.append(target.cpu().numpy())\n",
    "\n",
    "            \n",
    "# Transform using PCA to only 2 features \n",
    "# (probably a bad idea, but let's see what happens)\n",
    "#X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lin_clf = SVC(decision_function_shape='ovr', C=10,gamma=0.00001)\n",
    "lin_clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "plot_decision_regions(X, Y, clf=lin_clf, legend=4, \n",
    "                      colors='red,green,blue,purple,cyan,black,mediumslateblue,magenta,mediumseagreen,plum',\n",
    "                      markers='square,circle,diamond,star,triangle_up,triangle_down,x,triangle_right,triangle_left,plus')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
